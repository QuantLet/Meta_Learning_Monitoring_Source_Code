{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "##########  DEPENDECIES ############\n",
    "#####################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm # type: ignore\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, date\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from sklearn.model_selection import KFold, train_test_split # type: ignore\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import utils.prep as pr\n",
    "import utils.eval as ev\n",
    "import utils.inference as infer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "#####################################\n",
    "############  CONSTANTS #############\n",
    "#####################################\n",
    "RS = 42\n",
    "\n",
    "MODEL = \"CodeT5\"\n",
    "BATCH_SIZE = 16\n",
    "DECODER_LENGTH = 20\n",
    "ENCODER_LENGTH = 30\n",
    "ANALYSIS_POSTFIX = f\"mined_{str(date.today())}\"\n",
    "DATE_STR = 20240721\n",
    "SEMANTIC_DRIFT = True\n",
    "model_name=\"Salesforce/codet5-base-multi-sum\"\n",
    "\n",
    "FULL_TRAIN_ARGS = {\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"DECODER_LENGTH\": DECODER_LENGTH,\n",
    "    \"ENCODER_LENGTH\": ENCODER_LENGTH,\n",
    "    \"MODEL\": MODEL,\n",
    "    \"SEQ_TRAINER_ARGS\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": [0, 1, 3, 5, 8, 10, 16],\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 6e-6,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"save_strategy\": \"no\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": False,\n",
    "        \"output_dir\" : 'reports/results',\n",
    "        \"logging_dir\" : \"reports/logs\",\n",
    "    },\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conala data. Preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  (7405, 11)\n",
      "Test Data:  (2595, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2595/2595 [00:00<00:00, 106296.39 examples/s]\n",
      "Filter: 100%|██████████| 2595/2595 [00:00<00:00, 38936.04 examples/s]\n",
      "Map: 100%|██████████| 2595/2595 [00:01<00:00, 1914.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv(f\"../data/processed/conala/{DATE_STR}/conala_mined_clustered.csv\")\n",
    "\n",
    "if SEMANTIC_DRIFT:\n",
    "    dataset_4_cl = dataset[dataset.cluster==4].sample(n=2000, random_state=RS)\n",
    "    dataset_non_4_cl = dataset[dataset.cluster!=4].sample(n=8000, random_state=RS)\n",
    "\n",
    "    qids_4_cl = sorted(dataset_4_cl.question_id.unique())\n",
    "    train_idx_4_cl, test_idx_4_cl = qids_4_cl[int(len(qids_4_cl)*0.9):], qids_4_cl[:int(len(qids_4_cl)*0.9)]\n",
    "\n",
    "    qids_non4_cl = sorted(dataset_non_4_cl.question_id.unique())\n",
    "    train_idx_non4_cl, test_idx_non4_cl = qids_non4_cl[:int(len(qids_non4_cl)*0.9)], qids_non4_cl[int(len(qids_non4_cl)*0.9):]\n",
    "\n",
    "    train_dataset_4cl = dataset_4_cl[dataset_4_cl.question_id.isin(train_idx_4_cl)]\n",
    "    test_dataset_4cl = dataset_4_cl[dataset_4_cl.question_id.isin(test_idx_4_cl)]\n",
    "\n",
    "    train_dataset_non4cl = dataset_non_4_cl[dataset_non_4_cl.question_id.isin(train_idx_non4_cl)]\n",
    "    test_dataset_non4cl = dataset_non_4_cl[dataset_non_4_cl.question_id.isin(test_idx_non4_cl)]\n",
    "\n",
    "    train_dataset = pd.concat([train_dataset_4cl, train_dataset_non4cl], axis=0).sample(frac=1, random_state=RS).reset_index(drop=True)\n",
    "    test_dataset = pd.concat([test_dataset_4cl, test_dataset_non4cl], axis=0).sample(frac=1, random_state=RS).reset_index(drop=True)\n",
    "    \n",
    "else:\n",
    "    qids = sorted(dataset.question_id.unique())\n",
    "    train_idx, test_idx = qids[:int(len(qids)*0.8)], qids[int(len(qids)*0.8):]\n",
    "    train_dataset = dataset[dataset.question_id.isin(train_idx)]\n",
    "    test_dataset = dataset[dataset.question_id.isin(test_idx)]\n",
    "\n",
    "\n",
    "print(\"Train Data: \", train_dataset.shape)\n",
    "print(\"Test Data: \", test_dataset.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataset.sample(frac=1, random_state=RS).reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_dataset.sample(frac=1, random_state=RS).reset_index(drop=True))\n",
    "\n",
    "test_data = pr.preprocess_dataset(test_dataset, tokenizer=tokenizer, intent_colum_name=\"intent\")\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df[\"id\"] = test_df.index\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "folds = KFold(n_splits=3, random_state=RS, shuffle=True)\n",
    "questions_list = np.array(list(set(train_dataset[\"question_id\"])))\n",
    "splits_obj = folds.split(questions_list)\n",
    "splits = []\n",
    "for i, (train_idxs, val_idxs) in enumerate(splits_obj):\n",
    "    print(f\"Fold {i}\")\n",
    "    splits.append([train_idxs, val_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 7405/7405 [00:00<00:00, 55875.66 examples/s]\n",
      "Filter: 100%|██████████| 7405/7405 [00:00<00:00, 68309.19 examples/s]\n",
      "Filter: 100%|██████████| 4936/4936 [00:00<00:00, 35925.20 examples/s]\n",
      "Filter: 100%|██████████| 4936/4936 [00:00<00:00, 36853.18 examples/s]\n",
      "Map: 100%|██████████| 4936/4936 [00:02<00:00, 2122.76 examples/s]\n",
      "Filter: 100%|██████████| 2469/2469 [00:00<00:00, 40691.80 examples/s]\n",
      "Filter: 100%|██████████| 2469/2469 [00:00<00:00, 44005.36 examples/s]\n",
      "Map: 100%|██████████| 2469/2469 [00:00<00:00, 2528.13 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EPOCH SET 0\n",
      "TRAINING EPOCHS 0\n",
      "LOADING MODEL Salesforce/codet5-base-multi-sum\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING EPOCH SET 1\n",
      "TRAINING EPOCHS 1\n",
      "LOADING MODEL Salesforce/codet5-base-multi-sum\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='1234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 360/1234 00:46 < 01:52, 7.75 it/s, Epoch 0.29/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_results = {}\n",
    "for epoch_i, epoch_set in enumerate(sorted(FULL_TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"num_train_epochs\"])):\n",
    "    fold_results[epoch_set] = {}\n",
    "\n",
    "for i, (train_idxs, val_idxs) in enumerate(splits):\n",
    "\n",
    "    print(f\"Fold {i}\")\n",
    "    fold_dataset = DatasetDict({\n",
    "        \"train\": train_dataset.filter(lambda q_id: q_id[\"question_id\"] in questions_list[train_idxs]),\n",
    "        \"validation\": train_dataset.filter(lambda q_id: q_id[\"question_id\"] in questions_list[val_idxs]),\n",
    "    })\n",
    "    fold_train = pr.preprocess_dataset(fold_dataset[\"train\"], tokenizer=tokenizer, intent_colum_name=\"intent\")\n",
    "    fold_val = pr.preprocess_dataset(fold_dataset[\"validation\"], tokenizer=tokenizer, intent_colum_name=\"intent\")\n",
    "    \n",
    "\n",
    "    for epoch_i, epoch_set in enumerate(sorted(FULL_TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"num_train_epochs\"])):\n",
    "\n",
    "        fold_df = pd.DataFrame(fold_val)\n",
    "        print(f\"TRAINING EPOCH SET {epoch_set}\")\n",
    "\n",
    "        TRAIN_ARGS = copy.deepcopy(FULL_TRAIN_ARGS)\n",
    "        FOLD_MODEL_PATH = \"./tmp/\"\n",
    "\n",
    "        if epoch_set > 1: \n",
    "            TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"num_train_epochs\"] = epoch_set - latest_run_epoch\n",
    "        else:\n",
    "            TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"num_train_epochs\"] = epoch_set\n",
    "        \n",
    "        print(f'TRAINING EPOCHS {TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"num_train_epochs\"]}')\n",
    "\n",
    "        if epoch_set > 1: \n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(FOLD_MODEL_PATH)\n",
    "            print(f\"LOADING MODEL {FOLD_MODEL_PATH}\")\n",
    "        else: \n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "            print(f\"LOADING MODEL {model_name}\")\n",
    "\n",
    "        print(device)\n",
    "        model.to(device)\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "        compute_metrics = ev.compute_metric_with_params(tokenizer) \n",
    "\n",
    "        if not os.path.exists(f'reports/'): \n",
    "            os.mkdir(f'reports/')\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "                **TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"],\n",
    "            )\n",
    "        \n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=fold_train,\n",
    "            eval_dataset=fold_val,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        if epoch_set!=0:\n",
    "            trainer.train()\n",
    "\n",
    "        text = fold_val[\"input_sequence\"]\n",
    "        summaries = []\n",
    "        \n",
    "        if len(text)>1000:\n",
    "            \n",
    "            batch_size = 1000\n",
    "            n_batches = math.ceil(len(text)/batch_size)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "\n",
    "                batch_start_idx = batch*batch_size\n",
    "                batch_end_idx = batch*batch_size + batch_size\n",
    "\n",
    "                if batch==(n_batches-1):\n",
    "                    batch_end_idx = len(text)\n",
    "                summary = infer.generate_summary(text[batch_start_idx:batch_end_idx],\n",
    "                                                model,\n",
    "                                                tokenizer,\n",
    "                                                TRAIN_ARGS[\"ENCODER_LENGTH\"],\n",
    "                                                TRAIN_ARGS[\"DECODER_LENGTH\"])[1]\n",
    "                summaries.append(summary)\n",
    "\n",
    "            summaries = [sentence for summary_list in summaries for sentence in summary_list]\n",
    "            \n",
    "            fold_df[\"prediction\"] = summaries\n",
    "        else: \n",
    "            summaries = infer.generate_summary(text, \n",
    "                                               model,\n",
    "                                               tokenizer,\n",
    "                                               TRAIN_ARGS[\"ENCODER_LENGTH\"],\n",
    "                                               TRAIN_ARGS[\"DECODER_LENGTH\"])\n",
    "            fold_df[\"prediction\"] = summaries[1]\n",
    "\n",
    "\n",
    "        fold_df[\"rouge\"] = rouge.compute(predictions=fold_df[\"prediction\"], \n",
    "                    references=fold_df[\"output_sequence\"],\n",
    "                    use_stemmer=True, \n",
    "                    use_aggregator=False,\n",
    "                    rouge_types=[\"rouge1\"])[\"rouge1\"]\n",
    "        \n",
    "        fold_results[epoch_set][i] = fold_df\n",
    "        \n",
    "        ########## SAVE FOLD MODEL\n",
    "        if not os.path.exists(FOLD_MODEL_PATH): \n",
    "            os.mkdir(FOLD_MODEL_PATH)\n",
    "\n",
    "        trainer.save_model(FOLD_MODEL_PATH)\n",
    "\n",
    "        latest_run_epoch = epoch_set\n",
    "\n",
    "########## CONVERT TO DATAFRAME\n",
    "\n",
    "for epoch_i, (epoch_set) in enumerate(fold_results.keys()): \n",
    "    \n",
    "    for i, (k, f_df) in enumerate(fold_results[epoch_set].items()): \n",
    "        \n",
    "        f_df['fold'] = k\n",
    "        f_df['epoch_set'] = epoch_set\n",
    "\n",
    "        if (epoch_i==0 and i==0): \n",
    "            cv_df = f_df.copy()\n",
    "        else: \n",
    "            cv_df = pd.concat([cv_df, f_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SAVE THE FILE\n",
    "\n",
    "with open(f'reports/results/cv_result_{ANALYSIS_POSTFIX}.pickle', 'wb') as handle:\n",
    "    pickle.dump(cv_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LOAD CV RESULTS\n",
    "\n",
    "import pickle\n",
    "with open(f'reports/results/cv_result_{ANALYSIS_POSTFIX}.pickle', 'rb') as handle:\n",
    "    cv_df = pickle.load(handle)\n",
    "\n",
    "########## ROUGE PER SETTING\n",
    "\n",
    "print(\"Mean\")\n",
    "print(cv_df.groupby([\"epoch_set\"])[\"rouge\"].mean())\n",
    "\n",
    "print(\"STD\")\n",
    "print(cv_df.groupby(\"epoch_set\")[\"rouge\"].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Learn performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_two(X_train, y_train, model, X_val=None, y_val=None,  save=False): \n",
    "    global ANALYSIS_POSTFIX\n",
    "    \n",
    "    if model==\"lr\":\n",
    "        reg = LinearRegression().fit(X_train, y_train)\n",
    "    elif model ==\"svm\": \n",
    "        reg = SVR().fit(X_train, y_train)\n",
    "    elif model==\"rf\":\n",
    "        reg = RandomForestRegressor.fit(X_train, y_train)\n",
    "    elif model==\"lgbm\":\n",
    "        reg = LGBMRegressor()\n",
    "        reg.fit(X=X_train, y=y_train)\n",
    "    elif model==\"catboost\":\n",
    "        reg = CatBoostRegressor()\n",
    "        reg.fit(X=X_train, y=y_train)\n",
    "\n",
    "    if save:\n",
    "        with open(f'./models/reg_{model}_{ANALYSIS_POSTFIX}.pkl','wb') as f:\n",
    "            pickle.dump(reg, f)\n",
    "        return f'./models/reg_{model}_{ANALYSIS_POSTFIX}.pkl'\n",
    "    \n",
    "    else:\n",
    "        y_pred = reg.predict(X_val)\n",
    "        y_pred[y_pred<0] = 0\n",
    "        mae = mean_absolute_error(y_true=y_val, y_pred=y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(y_true=y_val, y_pred=y_pred))\n",
    "        return {\"pred\": y_pred, \"mae\": mae, \"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_models = [\"lr\", \"svm\", \"lgbm\", \"catboost\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "for test_fold in range(cv_df.fold.max()+1):\n",
    "    print(test_fold)\n",
    "\n",
    "    # Prepare the input data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(cv_df.loc[cv_df.fold!=test_fold, \"input_sequence\"])\n",
    "    X_train_column_sparse = pd.get_dummies(cv_df.loc[cv_df.fold!=test_fold, \"epoch_set\"], sparse=True).sparse.to_coo().tocsr()\n",
    "    X_train = hstack([X_train_column_sparse, X_train_tfidf])\n",
    "    y_train = cv_df.loc[cv_df.fold!=test_fold, \"rouge\"]\n",
    "    \n",
    "    X_val_tfidf = vectorizer.transform(cv_df.loc[cv_df.fold==test_fold, \"input_sequence\"])\n",
    "    X_val_column_sparse = pd.get_dummies(cv_df.loc[cv_df.fold==test_fold, \"epoch_set\"], sparse=True).sparse.to_coo().tocsr()\n",
    "    X_val = hstack([X_val_column_sparse, X_val_tfidf])\n",
    "    y_val = cv_df.loc[cv_df.fold==test_fold, \"rouge\"]\n",
    "\n",
    "    results[test_fold] = {}\n",
    "    for model in t_models:\n",
    "        print(model)\n",
    "        preds_df = step_two(X_train=X_train,\n",
    "                            y_train=y_train,\n",
    "                            X_val=X_val,\n",
    "                            y_val=y_val,\n",
    "                            model=model)\n",
    "        cv_df.loc[cv_df.fold==test_fold, f\"{model}_perf_hat\"] = preds_df[\"pred\"]\n",
    "        results[test_fold][model] = preds_df\n",
    "\n",
    "cv_df = cv_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby(\"epoch_set\").lgbm_perf_hat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby(\"epoch_set\").catboost_perf_hat.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the file\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model in t_models:\n",
    "    model_results[model]= {}\n",
    "    model_results[model][\"rmse\"] = []\n",
    "    model_results[model][\"mae\"] = [] \n",
    "\n",
    "    for fold in range(3):\n",
    "    \n",
    "        model_results[model][\"mae\"].append(results[fold][model][\"mae\"])\n",
    "        model_results[model][\"rmse\"].append(results[fold][model][\"rmse\"])\n",
    "    \n",
    "    model_results[model][\"rmse_avg\"] = np.array(model_results[model][\"rmse\"]).mean()\n",
    "    model_results[model][\"mae_avg\"] = np.array(model_results[model][\"mae\"]).mean()\n",
    "\n",
    "for model in t_models:\n",
    "    print(model)\n",
    "    print(\"RMSE \", model_results[model][\"rmse_avg\"])\n",
    "    print(\"MAE \",model_results[model][\"mae_avg\"])\n",
    "    print(\"\\n\")\n",
    "\n",
    "with open(f'reports/results/cd_df_with_s2_{ANALYSIS_POSTFIX}.pickle', 'wb') as handle:\n",
    "    pickle.dump(cv_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO SAVE THE VECTORIZER AND STEP 2 MODELS\n",
    "\n",
    "with open(f'reports/results/cd_df_with_s2_{ANALYSIS_POSTFIX}.pickle', 'rb') as handle:\n",
    "    cv_df = pickle.load(handle)\n",
    "\n",
    "# TRAIN ON ALL PREDICTIONS AT ONCE\n",
    "\n",
    "t_models = [\"lr\", \"svm\", \"lgbm\", \"catboost\"]\n",
    "\n",
    "# Prepare the input data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(cv_df.loc[:, \"input_sequence\"])\n",
    "X_train_column_sparse = pd.get_dummies(cv_df.loc[:, \"epoch_set\"], sparse=True).sparse.to_coo().tocsr()\n",
    "X_train = hstack([X_train_column_sparse, X_train_tfidf])\n",
    "y_train = cv_df.loc[:, \"rouge\"]\n",
    "    \n",
    "with open(f\"./models/vectorizer_{ANALYSIS_POSTFIX}.pkl\", \"wb\") as file:\n",
    "    pickle.dump(vectorizer, file, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "      \n",
    "for model in t_models:\n",
    "    print(model)\n",
    "    preds_df = step_two(X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        model=model,\n",
    "                        save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
