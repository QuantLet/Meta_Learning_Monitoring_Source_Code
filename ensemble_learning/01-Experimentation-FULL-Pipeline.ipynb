{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "##########  DEPENDECIES ############\n",
    "#####################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm # type: ignore\n",
    "from datetime import date\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import utils.prep as pr\n",
    "import utils.eval as ev\n",
    "import utils.inference as infer\n",
    "from utils.sampling import create_splits, prep_cv_validation\n",
    "from utils.training import cv_cluster_set, cv_training_epochs_sets, test_cluster_set\n",
    "from utils.training import results_dict_todf, cv_step_2, full_step_2, test_training_epochs_sets\n",
    "from utils.inference import meta_predict, create_ensemble_map, ensemble_compute\n",
    "\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "#####################################\n",
    "############  CONSTANTS #############\n",
    "#####################################\n",
    "RS = 42\n",
    "\n",
    "MODEL = \"CodeT5\"\n",
    "BATCH_SIZE = 16\n",
    "DECODER_LENGTH = 15\n",
    "ENCODER_LENGTH = 15\n",
    "DATE_STR = 20240721\n",
    "model_name=\"Salesforce/codet5-base-multi-sum\"\n",
    "\n",
    "FULL_TRAIN_ARGS = {\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"DECODER_LENGTH\": DECODER_LENGTH,\n",
    "    \"ENCODER_LENGTH\": ENCODER_LENGTH,\n",
    "    \"MODEL\": MODEL,\n",
    "    \"SEQ_TRAINER_ARGS\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": list(range(2)),\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 6e-6,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"save_strategy\": \"no\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": False,\n",
    "        \"output_dir\" : 'reports/results',\n",
    "        \"logging_dir\" : \"reports/logs\",\n",
    "    },\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conala data. Preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    \"DATE_STR\" : \"20240721\",\n",
    "    \"RS\" : 42,\n",
    "    \"DRIFT_TYPE\" : \"sudden\",\n",
    "    \"NFOLD\" : 3,\n",
    "    \"FULL_TRAIN_ARGS\" : FULL_TRAIN_ARGS,\n",
    "    \"MODEL_NAME\" : model_name,\n",
    "    \"CLUSTER_EPOCHS\" : 2,\n",
    "}\n",
    "experiment_config[\"ANALYSIS_POSTFIX\"] = f\"mined_{experiment_config['DRIFT_TYPE']}_{str(date.today())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dict = create_splits(experiment_config=experiment_config, tokenizer=tokenizer, test=False)\n",
    "train_dataset, test_data, test_df = sampling_dict[\"train_data\"], sampling_dict[\"test_data\"], sampling_dict[\"test_df\"]\n",
    "\n",
    "splits, questions_list = prep_cv_validation(train_dataset=train_dataset, \n",
    "                            experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = cv_training_epochs_sets(experiment_config=experiment_config,\n",
    "                            splits=splits,\n",
    "                            questions_list=questions_list,\n",
    "                            train_dataset=train_dataset,\n",
    "                            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'reports/results/foldresult_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'wb') as handle:\n",
    "    pickle.dump(fold_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'reports/results/foldresult_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'rb') as handle:\n",
    "#    fold_results = pickle.load(handle)\n",
    "\n",
    "with open(f'reports/results/foldresult_mined_sudden_2024-08-10.pickle', 'rb') as handle:\n",
    "    fold_results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cluster_idx in [1, 4, 3]:\n",
    "    fold_results = cv_cluster_set(experiment_config=experiment_config,\n",
    "                                    splits=splits,\n",
    "                                    questions_list=questions_list,\n",
    "                                    train_dataset=train_dataset,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    fold_results=fold_results,\n",
    "                                    cluster_id=cluster_idx)\n",
    "\n",
    "cv_df = results_dict_todf(fold_results)\n",
    "\n",
    "########## SAVE THE FILE\n",
    "\n",
    "with open(f'reports/results/cv_result_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'wb') as handle:\n",
    "    pickle.dump(cv_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results['cluster_1'][0]['rouge'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby(\"model_set\").rouge.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean\")\n",
    "print(cv_df.groupby([\"model_set\"])[\"rouge\"].mean())\n",
    "\n",
    "print(\"STD\")\n",
    "print(cv_df.groupby(\"model_set\")[\"rouge\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LOAD CV RESULTS\n",
    "\n",
    "import pickle\n",
    "with open(f'reports/results/cv_result_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'rb') as handle:\n",
    "    cv_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Learn performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_df, model_results = cv_step_2(experiment_config=experiment_config, cv_df=cv_df)\n",
    "\n",
    "with open(f'reports/results/s2_model_results_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'reports/results/cd_df_with_s2_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'wb') as handle:\n",
    "    pickle.dump(cv_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reports/results/cd_df_with_s2_mined_sudden_2024-08-15.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# print(\"Mean\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(cv_df.groupby([\"model_set\"])[\"rouge\"].mean())\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m### TO SAVE THE VECTORIZER AND STEP 2 MODELS\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreports/results/cd_df_with_s2_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mexperiment_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mANALYSIS_POSTFIX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m     10\u001b[0m     cv_df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(handle)\n",
      "File \u001b[0;32m~/.conda/envs/ensemble/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reports/results/cd_df_with_s2_mined_sudden_2024-08-15.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Mean\")\n",
    "print(cv_df.groupby([\"model_set\"])[\"rouge\"].mean())\n",
    "\n",
    "print(\"STD\")\n",
    "print(cv_df.groupby(\"model_set\")[\"rouge\"].std())\n",
    "\n",
    "### TO SAVE THE VECTORIZER AND STEP 2 MODELS\n",
    "\n",
    "with open(f'reports/results/cd_df_with_s2_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'rb') as handle:\n",
    "    cv_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_step_2(cv_df=cv_df, \n",
    "            experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_dict = create_splits(experiment_config=experiment_config, tokenizer=tokenizer, test=True)\n",
    "train_dataset, test_data, test_df = sampling_dict[\"train_data\"], sampling_dict[\"test_data\"], sampling_dict[\"test_df\"]\n",
    "\n",
    "splits, questions_list = prep_cv_validation(train_dataset=train_dataset, \n",
    "                            experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"reports/results/cd_df_with_s2_{experiment_config['ANALYSIS_POSTFIX']}.pickle\", \"rb\") as handle:\n",
    "    cv_resutls = pickle.load(handle)\n",
    "\n",
    "base_models_list = list(cv_resutls.model_set.unique())\n",
    "base_models_list.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_preds_df = meta_predict(experiment_config=experiment_config, \n",
    "                    test_df=test_df,\n",
    "                    base_models_names=base_models_list,\n",
    "                    t_models=[\"svm\", \"catboost\"])\n",
    "\n",
    "########## SAVE THE FILE\n",
    "\n",
    "with open(f'reports/results/test_results_s2_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'wb') as handle:\n",
    "    pickle.dump(meta_preds_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'reports/results/test_results_s2_mined_sudden_2024-08-11.pickle', 'rb') as handle:\n",
    "    meta_preds_df = pickle.load(handle)\n",
    "\n",
    "with open(f'reports/results/test_results_s2_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'rb') as handle:\n",
    "    meta_preds_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_preds_df.groupby(\"model_set\").svm_preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_ensemble_map, ensemble_val_estim = create_ensemble_map(meta_preds_df=meta_preds_df, \n",
    "                                                                t_model_name=\"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result_df = test_training_epochs_sets(experiment_config=experiment_config,\n",
    "                            test_df=test_df,\n",
    "                            test_data=test_data,\n",
    "                            train_data=train_dataset,\n",
    "                            tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'reports/results/test_results_df_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_result_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'reports/results/test_results_df_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'rb') as handle:\n",
    "#    test_result_df = pickle.load(handle)\n",
    "\n",
    "with open(f'reports/results/test_results_df_mined_sudden_2024-08-11.pickle', 'rb') as handle:\n",
    "    test_result_df = pickle.load(handle)\n",
    "\n",
    "test_result_df = test_result_df.rename(columns={\"epoch_set\": \"model_set\"})\n",
    "\n",
    "for cluster_idx in [1, 4, 3]:\n",
    "    test_result_dict = test_cluster_set(experiment_config=experiment_config,\n",
    "                                    test_df=test_df,\n",
    "                                    test_data=test_data,\n",
    "                                    tokenizer=tokenizer,\n",
    "                                    results_df=test_result_df,\n",
    "                                    cluster_id=cluster_idx)\n",
    "\n",
    "#test_result_df = results_dict_todf(test_result_dict)\n",
    "\n",
    "########## SAVE THE FILE\n",
    "\n",
    "with open(f'reports/results/test_results_df_{experiment_config[\"ANALYSIS_POSTFIX\"]}.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_result_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENSEMBLE COMPUTE\n",
    "test_result_df = ensemble_compute(test_result_df=test_result_df,\n",
    "                                  optimal_ensemble_map=optimal_ensemble_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## ROUGE PER SETTING\n",
    "\n",
    "print(\"Mean\")\n",
    "print(test_result_df.groupby(\"model_set\")[\"rouge\"].mean())\n",
    "\n",
    "print(\"STD\")\n",
    "print(test_result_df.groupby(\"model_set\")[\"rouge\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.opt_es_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
