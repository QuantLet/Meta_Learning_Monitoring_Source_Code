{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install bertviz\n",
    "#%pip install ipywidgets\n",
    "#%pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from bertviz import model_view, head_view\n",
    "from transformers import AutoTokenizer, utils, AutoModelForSeq2SeqLM\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lime import lime_text\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils.int_training import vectorize_input_df, train_classifier, classifier_inference\n",
    "\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "ANALYSIS_POSTFIX = \"mined_sudden_2024-08-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../ensemble_learning/reports/results/{ANALYSIS_POSTFIX}/cv_results.pickle\", \"rb\") as handle:\n",
    "    cv_predictions = pickle.load(handle)\n",
    "\n",
    "with open(f\"../ensemble_learning/reports/results/{ANALYSIS_POSTFIX}/test_results.pickle\", \"rb\") as handle:\n",
    "    test_predictions = pickle.load(handle)\n",
    "\n",
    "with open(f\"../ensemble_learning/reports/results/{ANALYSIS_POSTFIX}/s2_model_results.pickle\", \"rb\") as handle:\n",
    "    s2_predictions = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/codet5-base-multi-sum\"\n",
    "input_text = \"sum(d * 10 ** i for i, d in enumerate(x[::-1]))\"  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# PREPARE AN EXAMPLE\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "generated_ids = model.generate(input_ids, max_length=20)\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "encoder_input_ids = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    decoder_input_ids = tokenizer(output_text, return_tensors=\"pt\", add_special_tokens=True).input_ids\n",
    "outputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "encoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n",
    "decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
    "print(decoder_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view(\n",
    "    encoder_attention=outputs.encoder_attentions,\n",
    "    decoder_attention=outputs.decoder_attentions,\n",
    "    cross_attention=outputs.cross_attentions,\n",
    "    encoder_tokens= encoder_text,\n",
    "    decoder_tokens = decoder_text,\n",
    "    display_mode=\"light\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "explainer = LimeTextExplainer(class_names=['non-acceptable', 'acceptable'])\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "res = vectorize_input_df(df=cv_predictions, vectorizer=vectorizer, fit=True, acc_rouge=0.15)\n",
    "X, y, vectorizer = res[\"X\"], res[\"y\"], res[\"vectorizer\"]\n",
    "\n",
    "classifier = train_classifier(X, y)\n",
    "\n",
    "features = list(vectorizer.get_feature_names_out())\n",
    "features = list(cv_predictions.model_set.unique()) + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importance = classifier.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(features)[sorted_idx])\n",
    "plt.title('Feature Importance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_obs(input_texts, cv_predictions):\n",
    "\n",
    "    global vectorizer\n",
    "\n",
    "    if isinstance(input_texts, str):\n",
    "        input_texts = [input_texts]\n",
    "\n",
    "    dummy_df = cv_predictions.groupby(\"model_set\").head(1).reset_index(drop=True)[[\"input_sequence\", \"model_set\", \"catboost_perf_hat\"]]\n",
    "    for i_text in input_texts:\n",
    "        row_copy = dummy_df.iloc[[-1]].copy()\n",
    "        row_copy[\"input_sequence\"] = i_text\n",
    "        row_copy[\"model_set\"] = 0\n",
    "\n",
    "        dummy_df = pd.concat([dummy_df, row_copy], axis=0)\n",
    "\n",
    "    vectorized = vectorize_input_df(df=dummy_df, vectorizer=vectorizer, fit=False, acc_rouge=0.15)[\"X\"]\n",
    "    print(len(input_texts))\n",
    "    X = vectorized[-len(input_texts):]\n",
    "    return X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_classifier(txt):\n",
    "  \n",
    "  global classfier \n",
    "  global cv_predictions \n",
    "  txt = vectorize_obs(input_texts=txt, cv_predictions=cv_predictions)\n",
    "  probs = classifier.predict_proba(txt)\n",
    "  return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = explainer.explain_instance(\"\"\"model_0 trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                data_collator=data_collator,\n",
    "                train_dataset=fold_train,\n",
    "                eval_dataset=fold_val,\n",
    "                tokenizer=tokenizer,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\"\"\", lime_classifier,  num_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl.show_in_notebook(text=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
