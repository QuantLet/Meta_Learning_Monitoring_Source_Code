{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.training' from '/usr/net/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/../src/training.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src\n",
    "\n",
    "importlib.reload(src)\n",
    "\n",
    "from src.data_prep_utils import (  # noqa: E402\n",
    "    conala_to_time_batches,\n",
    "    load_time_sorted_conala,\n",
    "    prep_for_hf,\n",
    ")\n",
    "\n",
    "importlib.reload(src.data_prep_utils)\n",
    "\n",
    "from src.model import load_model_tok\n",
    "\n",
    "importlib.reload(src.model)\n",
    "\n",
    "\n",
    "from src.training import train_evaluate, dd_analysis\n",
    "importlib.reload(src.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"CodeT5\"\n",
    "TRAIN_N = 900\n",
    "BATCH_SIZE = 35\n",
    "DECODER_LENGTH = 20\n",
    "ENCODER_LENGTH = 15\n",
    "\n",
    "TRAIN_ARGS = {\n",
    "    \"TRAIN_N\": TRAIN_N,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"DECODER_LENGTH\": DECODER_LENGTH,\n",
    "    \"ENCODER_LENGTH\": ENCODER_LENGTH,\n",
    "    \"MODEL\": MODEL,\n",
    "    \"SEQ_TRAINER_ARGS\": {\n",
    "        \"output_dir\": f\"reports/{MODEL}_{TRAIN_N}_{BATCH_SIZE}/results\",\n",
    "        \"num_train_epochs\": 2,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"logging_dir\": f\"reports/{MODEL}_{TRAIN_N}_{BATCH_SIZE}/logs\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": False,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique questions: 2074\n",
      "Sort Question IDs\n",
      "Create t=0 training sample\n"
     ]
    }
   ],
   "source": [
    "df = load_time_sorted_conala(\"../data/raw/conala\")  # noqa: PD901\n",
    "df = conala_to_time_batches(df, TRAIN_N, BATCH_SIZE)  # noqa: PD901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_tok(MODEL, freeze=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f16abe981a47e8ac65bb7df3819677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation for time step 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050747eecfc342cc956e8123c85dcf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='714' max='714' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [714/714 01:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.759800</td>\n",
       "      <td>3.793849</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>11.976200</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.969900</td>\n",
       "      <td>0.970300</td>\n",
       "      <td>392</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.802600</td>\n",
       "      <td>3.799844</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>12.333300</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.987600</td>\n",
       "      <td>399</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "eval() missing 1 required positional argument: 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/prepare_conala.ipynb Zelle 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunnstern.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/prepare_conala.ipynb#Y154sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m loss_df \u001b[39m=\u001b[39m dd_analysis(df, tokenizer\u001b[39m=\u001b[39;49mtokenizer,model\u001b[39m=\u001b[39;49mmodel, training_args\u001b[39m=\u001b[39;49mTRAIN_ARGS)\n",
      "File \u001b[0;32m/usr/net/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/../src/training.py:145\u001b[0m, in \u001b[0;36mdd_analysis\u001b[0;34m(df, tokenizer, model, training_args)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m    144\u001b[0m     trainer \u001b[39m=\u001b[39m train(train_data, validation_data, training_args, model, tokenizer)\n\u001b[0;32m--> 145\u001b[0m trainer, results, analysis_name \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m(trainer, train_data, validation_data, training_args, model, tokenizer)\n\u001b[1;32m    146\u001b[0m loss\u001b[39m.\u001b[39mappend(results[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m rouge_1\u001b[39m.\u001b[39mappend(results[\u001b[39m\"\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: eval() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "loss_df = dd_analysis(df, tokenizer=tokenizer,model=model, training_args=TRAIN_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a94cb19652d4b53933a70d00e613574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d0c21e6c3645fc993fbe7233c1fe78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      5.064        0.181        0.028        0.154           0.154   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0       0.02        10.571  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='714' max='714' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [714/714 01:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.762300</td>\n",
       "      <td>3.803319</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>366</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.795100</td>\n",
       "      <td>3.768154</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.328400</td>\n",
       "      <td>11.595200</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.946600</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>383</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0      3.768        0.354        0.115         0.33           0.328   \n",
      "\n",
      "   eval_bleu  eval_gen_len  \n",
      "0      0.042        11.595  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer_seq2seq.Seq2SeqTrainer at 0x7f83f0fe3e50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evaluate(train_data, validation_data, TRAIN_ARGS, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dd_analysis' from 'src.training' (/usr/net/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/../src/training.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/RDC/zinovyee.hub/H:/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/prepare_conala.ipynb Zelle 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunnstern.wiwi.hu-berlin.de/home/RDC/zinovyee.hub/H%3A/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/prepare_conala.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m dd_analysis\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'dd_analysis' from 'src.training' (/usr/net/zinovyee.hub/IRTG/MLSC/MLSC_DD/notebooks/../src/training.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encode_code",
   "language": "python",
   "name": "encode_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
