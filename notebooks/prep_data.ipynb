{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import umap\n",
    "import pickle\n",
    "import importlib\n",
    "import os\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src\n",
    "\n",
    "importlib.reload(src)\n",
    "\n",
    "from src.data_prep_utils import (  # noqa: E402\n",
    "    load_time_sorted_conala,\n",
    "    time_batches\n",
    ")\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "DATE_STR = \"20240327\"\n",
    "CLUSTER_N = 10\n",
    "N_BATCHES = 65\n",
    "TRAIN_SIZE = 379\n",
    "RS = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_time_sorted_conala(\"../data/raw/conala\")\n",
    "dataset.loc[dataset.rewritten_intent.isna(), \"rewritten_intent\"] = dataset.loc[dataset.rewritten_intent.isna(), \"intent\"]\n",
    "dataset['idx'] = dataset.index \n",
    "\n",
    "if not os.path.exists(f\"../data/processed/conala/{DATE_STR}/temporal/\"):\n",
    "    drift_types = [\"temporal\", \"recurring\", \"gradual\", \"sudden\"]\n",
    "    for drift_t in drift_types:\n",
    "        if not os.path.exists(f\"../data/processed/conala/{DATE_STR}/{drift_t}\"):\n",
    "            os.makedirs(f\"../data/processed/conala/{DATE_STR}/{drift_t}\")\n",
    "\n",
    "    # TEMPORAL DRIFT (AS IS OBSERVED IN THE DATA)\n",
    "\n",
    "    dataset, batch_size = time_batches(dataset, TRAIN_SIZE, N_BATCHES)\n",
    "\n",
    "    for batch in dataset.time_batch.unique():\n",
    "        print(f\"Batch {batch}\")\n",
    "        \n",
    "        batch_df = dataset[dataset.time_batch == batch]\n",
    "        batch_df.to_csv(f\"../data/processed/conala/{DATE_STR}/temporal/conala_batch_{batch}.csv\", index=False)\n",
    "    \n",
    "    dataset.to_csv(f\"../data/processed/conala/{DATE_STR}/temporal/full.csv\", index=False)\n",
    "else: \n",
    "    print(\"Temporal batches already created\")\n",
    "    batch_size = pd.read_csv(f\"../data/processed/conala/{DATE_STR}/temporal/conala_batch_1.csv\").shape[0]\n",
    "    print(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"../data/processed/conala/{DATE_STR}/conala_clustered.csv\"):\n",
    "    # TOPIC MODELING\n",
    "    # we do the topic modeling based on the semantic meaning of the intent\n",
    "\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Our sentences to encode\n",
    "    sentences = dataset.rewritten_intent.values\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    # Print the embeddings\n",
    "    for sentence, embedding in zip(sentences, embeddings):\n",
    "        print(\"Sentence:\", sentence)\n",
    "        print(\"Embedding:\", embedding)\n",
    "        print(\"\")\n",
    "else: \n",
    "    print(\"Embeddings already created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CLUSTER_N:\n",
    "    km_silhouette = []\n",
    "    km_db = []\n",
    "    n_clusters = [*range(3,50)]\n",
    "\n",
    "\n",
    "    for i in n_clusters:\n",
    "        cluster = KMeans(n_clusters=i,          \n",
    "                        random_state=42).fit(embeddings)\n",
    "        \n",
    "        preds = cluster.predict(embeddings)\n",
    "        \n",
    "        s_score = silhouette_score(embeddings, preds)\n",
    "        db_score = davies_bouldin_score(embeddings, preds)\n",
    "        km_silhouette.append(s_score)\n",
    "        km_db.append(db_score)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.scatter(x=[i for i in n_clusters], y=km_silhouette, s=150, edgecolor='k')\n",
    "    plt.xlabel(\"Number of clusters\", fontsize=14)\n",
    "    plt.ylabel(\"Silhouette score\", fontsize=15)\n",
    "    plt.xticks([10, 20, 30, 40, 50], fontsize=14)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "# WE IDENTIFIED 10 GROUPS AS THE OPTIMAL NUMBER OF CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"../data/processed/conala/{DATE_STR}/conala_clustered.csv\"):\n",
    "    cluster = KMeans(n_clusters=CLUSTER_N,          \n",
    "                        random_state=RS).fit(embeddings)\n",
    "        \n",
    "    preds = cluster.predict(embeddings)\n",
    "    dataset[\"cluster\"] = preds\n",
    "\n",
    "    # SAVE DATASET AND EMEDDINGS\n",
    "    dataset.to_csv(f\"../data/processed/conala/{DATE_STR}/conala_clustered.csv\", index=False)\n",
    "    with open(f\"../data/processed/conala/{DATE_STR}/conala_embeddings.pkl\", \"wb\") as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "    if False:\n",
    "        for cluster_id in sorted(dataset.cluster.unique()):\n",
    "            plt.figure(figsize=(10,4))\n",
    "            sns.histplot(data=dataset[dataset.cluster == cluster_id], x=\"time_batch\")\n",
    "            plt.title(f\"Cluster {cluster_id}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = string interactions <br>\n",
    "1 = dictionary operations <br>\n",
    "2 = date operations <br>\n",
    "3 = pandas and df operations <br>\n",
    "4 = os, htto, process operations <br>\n",
    "5 = working with numbers, casting, precisions <br>\n",
    "6 = numpy interactions <br>\n",
    "7 = list, zip, range interactions, sorting etc <br>\n",
    "8 = string encodings <br>\n",
    "9 = working with files <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Drift Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f\"../data/processed/conala/{DATE_STR}/conala_clustered.csv\")\n",
    "with open(f\"../data/processed/conala/{DATE_STR}/conala_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_percentages = dataset.cluster[-dataset.cluster.isin((4,7,9))].value_counts(normalize=True).sort_values(ascending=True)\n",
    "cluster_percentages_dict = cluster_percentages.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADUAL DRIFT \n",
    "# the 7 out of 10 topics are present during the initial training, new three topics are introduced gradually\n",
    "# 4,7,9 gradually\n",
    "gradual_to_sample = {}\n",
    "sampled = 0\n",
    "dataset[\"gradual_batch\"] = -1\n",
    "for i, cluster_id in enumerate(cluster_percentages.index): \n",
    "    print(cluster_id)\n",
    "    \n",
    "    to_sample = math.ceil(cluster_percentages_dict[cluster_id]*batch_size)\n",
    "    \n",
    "    if i==(len(cluster_percentages.index)-1):\n",
    "        to_sample = TRAIN_SIZE - sampled\n",
    "    print(to_sample)\n",
    "    sampled_idx = dataset[dataset.cluster == cluster_id].sample(n=to_sample, random_state=RS).index\n",
    "    dataset.loc[sampled_idx, \"gradual_batch\"] = 0\n",
    "    sampled += to_sample\n",
    "\n",
    "print(sampled, \" \", TRAIN_SIZE)\n",
    "\n",
    "dataset[\"gradual_new\"] = 0 \n",
    "dataset.loc[dataset.cluster.isin((4,7,9)), \"gradual_new\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_per_dict = {\n",
    "    11: 0.1,\n",
    "    12: 0.2,\n",
    "    13: 0.3,\n",
    "    14: 0.4,\n",
    "    15: 0.5,\n",
    "}\n",
    "\n",
    "for batch_id in range(1, N_BATCHES+1):\n",
    "    if batch_id < 11:\n",
    "        sampled_idx = dataset[(dataset[\"gradual_new\"]==0) & (dataset[\"gradual_batch\"] == -1)].sample(n=batch_size, random_state=RS).index\n",
    "        dataset.loc[sampled_idx, \"gradual_batch\"] = batch_id\n",
    "\n",
    "    elif batch_id in (11, 12, 13, 14, 15):\n",
    "        to_sample_new = math.ceil(batch_size * topic_per_dict[batch_id])\n",
    "        to_sample_old = batch_size - to_sample_new\n",
    "        sampled_old_idx = dataset[(dataset[\"gradual_new\"]==0) & (dataset[\"gradual_batch\"] == -1)].sample(n=to_sample_old, random_state=RS).index\n",
    "        dataset.loc[sampled_old_idx, \"gradual_batch\"] = batch_id\n",
    "        sampled_new_idx = dataset[(dataset[\"gradual_new\"]==1) & (dataset[\"gradual_batch\"] == -1)].sample(n=to_sample_new, random_state=RS).index\n",
    "        dataset.loc[sampled_new_idx, \"gradual_batch\"] = batch_id\n",
    "    elif batch_id == N_BATCHES: # last batch\n",
    "        sampled_idx = dataset[(dataset[\"gradual_batch\"] == -1)].index\n",
    "        dataset.loc[sampled_idx, \"gradual_batch\"] = batch_id\n",
    "    else:\n",
    "        sampled_idx = dataset[(dataset[\"gradual_batch\"] == -1)].sample(n=batch_size, random_state=RS).index\n",
    "        dataset.loc[sampled_idx, \"gradual_batch\"] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset.gradual_batch.unique():\n",
    "    print(f\"Batch {batch}\")\n",
    "    \n",
    "    batch_df = dataset[dataset.gradual_batch == batch]\n",
    "    batch_df.to_csv(f\"../data/processed/conala/{DATE_STR}/gradual/conala_batch_{batch}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECURRING DRIFT\n",
    "# the 9 out of 10 topics are present during the initial training, new three topics are introduced at three different time points\n",
    "# 4 recurring\n",
    "\n",
    "cluster_percentages = dataset.cluster.loc[-dataset.cluster.isin([4])].value_counts(normalize=True).sort_values(ascending=True)\n",
    "cluster_percentages_dict = cluster_percentages.to_dict()\n",
    "\n",
    "sampled = 0\n",
    "dataset[\"recurring_batch\"] = -1\n",
    "for i, cluster_id in enumerate(cluster_percentages.index): \n",
    "    to_sample = math.ceil(cluster_percentages_dict[cluster_id]*batch_size)\n",
    "    if i==(len(cluster_percentages.index)-1):\n",
    "        to_sample = TRAIN_SIZE - sampled\n",
    "\n",
    "    sampled_idx = dataset[dataset.cluster == cluster_id].sample(n=to_sample, random_state=RS).index\n",
    "    dataset.loc[sampled_idx, \"recurring_batch\"] = 0\n",
    "    sampled += to_sample\n",
    "\n",
    "print(sampled, \" \", TRAIN_SIZE)\n",
    "\n",
    "dataset[\"rec_new\"] = 0 \n",
    "dataset.loc[dataset.cluster.isin([4]), \"rec_new\"] = 1\n",
    "\n",
    "new_topic_batch = math.ceil(dataset[dataset[\"rec_new\"]==1].shape[0]/batch_size)\n",
    "\n",
    "recurrent_batch_ids = [10, 11, 12, 25, 26, 27, 28, 45, 46, 47, 48]\n",
    "max_batch = max(recurrent_batch_ids)\n",
    "\n",
    "\n",
    "for batch_id in range(1, N_BATCHES+1):\n",
    "    if batch_id in recurrent_batch_ids:\n",
    "        if batch_id==max_batch:\n",
    "            sampled_new_idx = dataset[(dataset[\"rec_new\"]==1) & (dataset[\"recurring_batch\"] == -1)].index\n",
    "            dataset.loc[sampled_new_idx, \"recurring_batch\"] = batch_id\n",
    "            to_sample_old = batch_size - len(sampled_new_idx)\n",
    "            sampled_old_idx = dataset[(dataset[\"rec_new\"]==0) & (dataset[\"recurring_batch\"] == -1)].sample(n=to_sample_old, random_state=RS).index\n",
    "            dataset.loc[sampled_old_idx, \"recurring_batch\"] = batch_id\n",
    "        else:\n",
    "            sampled_new_idx = dataset[(dataset[\"rec_new\"]==1) & (dataset[\"recurring_batch\"] == -1)].sample(n=batch_size, random_state=RS).index\n",
    "            dataset.loc[sampled_new_idx, \"recurring_batch\"] = batch_id\n",
    "    else:\n",
    "        if batch_id == N_BATCHES: # last batch\n",
    "            sampled_idx = dataset[(dataset[\"recurring_batch\"] == -1)].index\n",
    "            dataset.loc[sampled_idx, \"recurring_batch\"] = batch_id\n",
    "        else:\n",
    "            sampled_idx = dataset[(dataset[\"rec_new\"]==0) & (dataset[\"recurring_batch\"] == -1)].sample(n=batch_size, random_state=RS).index\n",
    "            dataset.loc[sampled_idx, \"recurring_batch\"] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset.recurring_batch.unique():\n",
    "    print(f\"Batch {batch}\")\n",
    "    \n",
    "    batch_df = dataset[dataset.recurring_batch == batch]\n",
    "    batch_df.to_csv(f\"../data/processed/conala/{DATE_STR}/recurring/conala_batch_{batch}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUDDEN TOPIC DRIFT\n",
    "# 9 topics are present during the initial training, 1 topic is introduced suddenly \n",
    "# 2 is introduced at the 10th time point\n",
    "\n",
    "cluster_percentages = dataset.cluster.loc[-dataset.cluster.isin([2])].value_counts(normalize=True).sort_values(ascending=True)\n",
    "cluster_percentages_dict = cluster_percentages.to_dict()\n",
    "\n",
    "sampled = 0\n",
    "dataset[\"sudden_batch\"] = -1\n",
    "for i, cluster_id in enumerate(cluster_percentages.index): \n",
    "    to_sample = math.ceil(cluster_percentages_dict[cluster_id]*batch_size)\n",
    "    if i==(len(cluster_percentages.index)-1):\n",
    "        to_sample = TRAIN_SIZE - sampled\n",
    "\n",
    "    sampled_idx = dataset[dataset.cluster == cluster_id].sample(n=to_sample, random_state=RS).index\n",
    "    dataset.loc[sampled_idx, \"sudden_batch\"] = 0\n",
    "    sampled += to_sample\n",
    "\n",
    "print(sampled, \" \", TRAIN_SIZE)\n",
    "\n",
    "dataset[\"sudden_new\"] = 0 \n",
    "dataset.loc[dataset.cluster.isin([2]), \"sudden_new\"] = 1\n",
    "\n",
    "new_topic_batch = math.ceil(dataset[dataset[\"sudden_new\"]==1].shape[0]/batch_size)\n",
    "\n",
    "sudden_batch_ids = [10, 11, 12]\n",
    "max_batch = max(sudden_batch_ids)\n",
    "\n",
    "\n",
    "for batch_id in range(1, N_BATCHES+1):\n",
    "    if batch_id in sudden_batch_ids:\n",
    "        if batch_id==max_batch:\n",
    "            sampled_new_idx = dataset[(dataset[\"sudden_new\"]==1) & (dataset[\"sudden_batch\"] == -1)].index\n",
    "            dataset.loc[sampled_new_idx, \"sudden_batch\"] = batch_id\n",
    "            to_sample_old = batch_size - len(sampled_new_idx)\n",
    "            sampled_old_idx = dataset[(dataset[\"sudden_new\"]==0) & (dataset[\"sudden_batch\"] == -1)].sample(n=to_sample_old, random_state=RS).index\n",
    "            dataset.loc[sampled_old_idx, \"sudden_batch\"] = batch_id\n",
    "        else:\n",
    "            sampled_new_idx = dataset[(dataset[\"sudden_new\"]==1) & (dataset[\"sudden_batch\"] == -1)].sample(n=batch_size, random_state=RS).index\n",
    "            dataset.loc[sampled_new_idx, \"sudden_batch\"] = batch_id\n",
    "    else:\n",
    "        if batch_id == N_BATCHES: # last batch\n",
    "            sampled_idx = dataset[(dataset[\"sudden_batch\"] == -1)].index\n",
    "            dataset.loc[sampled_idx, \"sudden_batch\"] = batch_id\n",
    "        else:\n",
    "            sampled_idx = dataset[(dataset[\"sudden_new\"]==0) & (dataset[\"sudden_batch\"] == -1)].sample(n=batch_size, random_state=RS).index\n",
    "            dataset.loc[sampled_idx, \"sudden_batch\"] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset.sudden_batch.unique():\n",
    "    print(f\"Batch {batch}\")\n",
    "    \n",
    "    batch_df = dataset[dataset.sudden_batch == batch]\n",
    "    batch_df.to_csv(f\"../data/processed/conala/{DATE_STR}/sudden/conala_batch_{batch}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(f\"../data/processed/conala/{DATE_STR}/all_drifts.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
