{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######### SETTINGS #########\n",
    "############################\n",
    "\n",
    "# Installations\n",
    "#%pip install rouge_score\n",
    "#%pip install absl\n",
    "#%pip install seaborn\n",
    "#%pip install transformers[torch]\n",
    "\n",
    "# Dependencies\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "sys.path.append(\"../\")\n",
    "from sklearn.model_selection import KFold\n",
    "import src\n",
    "\n",
    "importlib.reload(src)\n",
    "\n",
    "from src.data_prep_utils import (  # noqa: E402\n",
    "    conala_to_time_batches,\n",
    "    load_time_sorted_conala,\n",
    ")\n",
    "\n",
    "importlib.reload(src.data_prep_utils)\n",
    "\n",
    "\n",
    "from src.training import nd_inference, retraining, continual\n",
    "importlib.reload(src.training)\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from src.processing_utils import compute_metric_with_params, prepare_hg_ds\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import seaborn as sns\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# Constants\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "MODEL = \"CodeT5\"\n",
    "TRAIN_N = 330\n",
    "BATCH_SIZE = 15\n",
    "DECODER_LENGTH = 20\n",
    "ENCODER_LENGTH = 15\n",
    "RS = 42\n",
    "\n",
    "TRAIN_ARGS = {\n",
    "    \"TRAIN_N\": TRAIN_N,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"DECODER_LENGTH\": DECODER_LENGTH,\n",
    "    \"ENCODER_LENGTH\": ENCODER_LENGTH,\n",
    "    \"MODEL\": MODEL,\n",
    "    \"SEQ_TRAINER_ARGS\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"save_strategy\": \"no\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_name=\"Salesforce/codet5-base-multi-sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######## FUNCTIONS #########\n",
    "############################\n",
    "\n",
    "def prep_for_hf(df: pd.DataFrame) -> Dataset:\n",
    "    \n",
    "    \"\"\"Convert pandas dataframe to huggingface.\"\"\"\n",
    "    \n",
    "    df = df.rename(columns={\"snippet\": \"input_sequence\",  \n",
    "                    \"rewritten_intent\" : \"output_sequence\"})\n",
    "    df = df.loc[:, [\"input_sequence\", \"output_sequence\", \"idx\"]]  \n",
    "    df = df.sample(frac=1, random_state=RS)  \n",
    "    return df, Dataset.from_pandas(df)\n",
    "\n",
    "def batch_tokenize_preprocess(batch, tokenizer, max_input_length, max_output_length):\n",
    "\n",
    "    source = batch[\"input_sequence\"]\n",
    "    target = batch[\"output_sequence\"]\n",
    "\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\",\n",
    "        truncation=True, max_length=max_input_length\n",
    "    )\n",
    "\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\",\n",
    "        truncation=True, max_length=max_output_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return batch\n",
    "\n",
    "def generate_summary(test_samples, model, tokenizer, encoder_max_length, decoder_max_length):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"input_sequence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=decoder_max_length)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "def rouge_custom(prediction, reference): \n",
    "    splitted_reference = reference.lower().split()\n",
    "    matched = sum([word in prediction.lower().split() for word in splitted_reference])\n",
    "    return matched / len(splitted_reference)\n",
    "\n",
    "def bleu_custom(prediction, reference): \n",
    "    splitted_prediction = prediction.lower().split()\n",
    "    matched = sum([word in reference.lower().split() for word in splitted_prediction])\n",
    "    return matched / len(splitted_prediction)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds  = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metric_with_params(tokenizer, metrics_list=['rouge', 'bleu']):\n",
    "    def compute_metrics(eval_preds):\n",
    "\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # POST PROCESSING\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        results_dict = {}\n",
    "        for m in metrics_list:\n",
    "            metric = evaluate.load(m)\n",
    "\n",
    "            if m=='bleu':\n",
    "                result = metric.compute(\n",
    "                    predictions=decoded_preds, references=decoded_labels\n",
    "                )\n",
    "            elif m=='rouge':\n",
    "                result = metric.compute(\n",
    "                    predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "                )\n",
    "            result = {key: value for key, value in result.items() if key!='precisions'}\n",
    "\n",
    "            prediction_lens = [\n",
    "                np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "            ]\n",
    "            result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "            result = {k: round(v, 4) for k, v in result.items()}\n",
    "            results_dict.update(result)\n",
    "        return results_dict\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "###### CV LOOP PREP ########\n",
    "############################\n",
    "\n",
    "DATE_STR = \"20240327\"\n",
    "df = pd.read_csv(f\"../data/processed/conala/{DATE_STR}/all_drifts.csv\")\n",
    "df[\"t_batch\"] = df[\"time_batch\"]\n",
    "df.loc[df.rewritten_intent.isna(), \"rewritten_intent\"] = \"translate an ISO 8601 datetime string into a Python datetime object\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "\n",
    "full_train_idx = pd.Series(df.question_id.unique()).sample(n=1200, random_state=RS)\n",
    "test_idx = pd.Series(df.loc[~df.question_id.isin(full_train_idx), \"question_id\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "###### MODEL SETTINGS ######\n",
    "############################\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "compute_metrics = compute_metric_with_params(tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"output_dir\"] = f'reports/upper_bound/results'\n",
    "TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"logging_dir\"] = f'reports/upper_bound/logs'\n",
    "\n",
    "if not os.path.exists('reports/'): \n",
    "    os.mkdir('reports/')\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        **TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Fold: 0\n",
      "Preparing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing val data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/852 00:50 < 01:41, 5.56 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/137 00:00 < 00:24, 5.28 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######### CV TRAINING ######\n",
    "############################\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=RS, shuffle=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(full_train_idx.values)):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Fold: {fold}\")\n",
    "    \n",
    "    fold_results= {}\n",
    "\n",
    "    train_df = df.loc[df.question_id.isin(full_train_idx.iloc[train_idx]),:]\n",
    "    val_df = df.loc[df.question_id.isin(full_train_idx.iloc[val_idx]),:]\n",
    "\n",
    "    train_df, train_dataset = prep_for_hf(train_df)\n",
    "    val_df, val_dataset = prep_for_hf(val_df)\n",
    "\n",
    "    print(\"Preparing train data\")\n",
    "\n",
    "    train_data = train_dataset.map(\n",
    "            lambda batch: batch_tokenize_preprocess(\n",
    "                batch,\n",
    "                tokenizer=tokenizer,\n",
    "                max_input_length=ENCODER_LENGTH,\n",
    "                max_output_length=DECODER_LENGTH,\n",
    "            ),\n",
    "            batch_size=4,\n",
    "            batched=True,\n",
    "            #remove_columns=train_dataset.column_names,\n",
    "        )\n",
    "\n",
    "    print(\"Preparing val data\")\n",
    "\n",
    "    val_data = val_dataset.map(\n",
    "            lambda batch: batch_tokenize_preprocess(\n",
    "                batch,\n",
    "                tokenizer=tokenizer,\n",
    "                max_input_length=ENCODER_LENGTH,\n",
    "                max_output_length=DECODER_LENGTH,\n",
    "            ),\n",
    "            batch_size=4,\n",
    "            batched=True,\n",
    "            #remove_columns=train_dataset.column_names,\n",
    "        )\n",
    "\n",
    "    fold_trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    FOLD_MODEL_PATH = \"reports/no_drift/fold_model\"\n",
    "    if not os.path.exists(FOLD_MODEL_PATH): \n",
    "        os.mkdir(FOLD_MODEL_PATH)\n",
    "\n",
    "    print(\"Training\")\n",
    "    fold_trainer.train()\n",
    "    fold_trainer.save_model(FOLD_MODEL_PATH)\n",
    "\n",
    "    val_ground_truths = val_data[\"output_sequence\"]\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    print(\"Inference\")\n",
    "    # Fine-Tuned\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(FOLD_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FOLD_MODEL_PATH, skip_special_tokens=False)\n",
    "    fold_predictions_ft = generate_summary(val_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "    # Zero-Shot\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "    fold_predictions_zs = generate_summary(val_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "    # Rouge\n",
    "    print(\"Evaluation\")\n",
    "    fold_predictions_ft_rouge = rouge.compute(references=val_ground_truths, predictions=fold_predictions_ft, use_aggregator=False)[\"rouge1\"]\n",
    "    fold_predictions_zs_rouge = rouge.compute(references=val_ground_truths, predictions=fold_predictions_zs, use_aggregator=False)[\"rouge1\"]\n",
    "    \n",
    "    fold_results[\"input_sequence\"] = val_ground_truths\n",
    "    fold_results[\"fold_predictions_ft\"] = fold_predictions_ft\n",
    "    fold_results[\"fold_predictions_zs\"] = fold_predictions_zs\n",
    "    fold_results[\"fold_predictions_ft_rouge\"] = fold_predictions_ft_rouge\n",
    "    fold_results[\"fold_predictions_zs_rouge\"] = fold_predictions_zs_rouge\n",
    "\n",
    "    # Combine\n",
    "    fold_results = pd.DataFrame(fold_results)\n",
    "    fold_results[\"fold\"] = fold\n",
    "\n",
    "    if fold==0:\n",
    "        results = fold_results.copy()\n",
    "    else: \n",
    "        results = pd.concat([results, fold_results])\n",
    "\n",
    "results.to_csv(\"cv_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1263' max='1263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1263/1263 05:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.777400</td>\n",
       "      <td>3.518943</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>13.946400</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>5992</td>\n",
       "      <td>6881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.957600</td>\n",
       "      <td>3.384997</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>0.400800</td>\n",
       "      <td>14.249500</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>6289</td>\n",
       "      <td>6881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.251900</td>\n",
       "      <td>3.390300</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>14.970400</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.972700</td>\n",
       "      <td>0.973100</td>\n",
       "      <td>6696</td>\n",
       "      <td>6881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######## FULL TRAINING #####\n",
    "############################\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "train_df = df.loc[df.question_id.isin(full_train_idx),:]\n",
    "test_df = df.loc[df.question_id.isin(test_idx),:]\n",
    "\n",
    "train_df, train_dataset = prep_for_hf(train_df)\n",
    "test_df, test_dataset = prep_for_hf(test_df)\n",
    "\n",
    "train_data = train_dataset.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_length=ENCODER_LENGTH,\n",
    "            max_output_length=DECODER_LENGTH,\n",
    "        ),\n",
    "        batch_size=4,\n",
    "        batched=True,\n",
    "        #remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "\n",
    "test_data = test_dataset.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_length=ENCODER_LENGTH,\n",
    "            max_output_length=DECODER_LENGTH,\n",
    "        ),\n",
    "        batch_size=4,\n",
    "        batched=True,\n",
    "        #remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "MODEL_PATH = f\"reports/upper_bound/saved_model\"\n",
    "if not os.path.exists(MODEL_PATH): \n",
    "    os.mkdir(MODEL_PATH)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######### INFERENCE ########\n",
    "############################\n",
    "\n",
    "test_ground_truths = test_data[\"output_sequence\"]\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, skip_special_tokens=False)\n",
    "test_df[\"predictions_ft\"] = generate_summary(test_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "test_df[\"predictions_zs\"] = generate_summary(test_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "############################\n",
    "######## EVALUATION ########\n",
    "############################\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "test_df[\"predictions_ft_rouge\"] = rouge.compute(references=test_ground_truths, predictions=test_df[\"predictions_ft\"].values, use_aggregator=False)[\"rouge1\"]\n",
    "test_df[\"predictions_zs_rouge\"] = rouge.compute(references=test_ground_truths, predictions=test_df[\"predictions_zs\"].values, use_aggregator=False)[\"rouge1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.278477188184076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4189334588957666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_df.predictions_zs_rouge.mean())\n",
    "test_df.predictions_ft_rouge.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"test_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_spek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
