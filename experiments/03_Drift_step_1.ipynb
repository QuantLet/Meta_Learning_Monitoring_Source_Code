{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######### SETTINGS #########\n",
    "############################\n",
    "\n",
    "# Installations\n",
    "#%pip install rouge_score\n",
    "#%pip install absl\n",
    "#%pip install seaborn\n",
    "#%pip install transformers[torch]\n",
    "\n",
    "# Dependencies\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "sys.path.append(\"../\")\n",
    "from sklearn.model_selection import KFold\n",
    "import src\n",
    "\n",
    "importlib.reload(src)\n",
    "\n",
    "from src.data_prep_utils import (  # noqa: E402\n",
    "    conala_to_time_batches,\n",
    "    load_time_sorted_conala,\n",
    ")\n",
    "\n",
    "importlib.reload(src.data_prep_utils)\n",
    "\n",
    "\n",
    "from src.training import nd_inference, retraining, continual\n",
    "importlib.reload(src.training)\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from src.processing_utils import compute_metric_with_params, prepare_hg_ds\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import seaborn as sns\n",
    "import evaluate\n",
    "\n",
    "\n",
    "# Constants\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "MODEL = \"CodeT5\"\n",
    "TRAIN_N = 330\n",
    "BATCH_SIZE = 15\n",
    "DECODER_LENGTH = 20\n",
    "ENCODER_LENGTH = 15\n",
    "RS = 42\n",
    "\n",
    "TRAIN_ARGS = {\n",
    "    \"TRAIN_N\": TRAIN_N,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"DECODER_LENGTH\": DECODER_LENGTH,\n",
    "    \"ENCODER_LENGTH\": ENCODER_LENGTH,\n",
    "    \"MODEL\": MODEL,\n",
    "    \"SEQ_TRAINER_ARGS\": {\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"label_smoothing_factor\": 0.1,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"logging_steps\": 100,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"save_strategy\": \"no\",\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"load_best_model_at_end\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "model_name=\"Salesforce/codet5-base-multi-sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/RDC/zinovyee.hub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######## FUNCTIONS #########\n",
    "############################\n",
    "\n",
    "def prep_for_hf(df: pd.DataFrame) -> Dataset:\n",
    "    \n",
    "    \"\"\"Convert pandas dataframe to huggingface.\"\"\"\n",
    "    \n",
    "    df = df.rename(columns={\"snippet\": \"input_sequence\",  \n",
    "                    \"rewritten_intent\" : \"output_sequence\"})\n",
    "    df = df.loc[:, [\"input_sequence\", \"output_sequence\", \"idx\"]]  \n",
    "    df = df.sample(frac=1, random_state=RS)  \n",
    "    return df, Dataset.from_pandas(df)\n",
    "\n",
    "def batch_tokenize_preprocess(batch, tokenizer, max_input_length, max_output_length):\n",
    "\n",
    "    source = batch[\"input_sequence\"]\n",
    "    target = batch[\"output_sequence\"]\n",
    "\n",
    "    source_tokenized = tokenizer(\n",
    "        source, padding=\"max_length\",\n",
    "        truncation=True, max_length=max_input_length\n",
    "    )\n",
    "\n",
    "    target_tokenized = tokenizer(\n",
    "        target, padding=\"max_length\",\n",
    "        truncation=True, max_length=max_output_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return batch\n",
    "\n",
    "def generate_summary(test_samples, model, tokenizer, encoder_max_length, decoder_max_length):\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"input_sequence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=decoder_max_length)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "def rouge_custom(prediction, reference): \n",
    "    splitted_reference = reference.lower().split()\n",
    "    matched = sum([word in prediction.lower().split() for word in splitted_reference])\n",
    "    return matched / len(splitted_reference)\n",
    "\n",
    "def bleu_custom(prediction, reference): \n",
    "    splitted_prediction = prediction.lower().split()\n",
    "    matched = sum([word in reference.lower().split() for word in splitted_prediction])\n",
    "    return matched / len(splitted_prediction)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds  = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metric_with_params(tokenizer, metrics_list=['rouge', 'bleu']):\n",
    "    def compute_metrics(eval_preds):\n",
    "\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # POST PROCESSING\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        results_dict = {}\n",
    "        for m in metrics_list:\n",
    "            metric = evaluate.load(m)\n",
    "\n",
    "            if m=='bleu':\n",
    "                result = metric.compute(\n",
    "                    predictions=decoded_preds, references=decoded_labels\n",
    "                )\n",
    "            elif m=='rouge':\n",
    "                result = metric.compute(\n",
    "                    predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "                )\n",
    "            result = {key: value for key, value in result.items() if key!='precisions'}\n",
    "\n",
    "            prediction_lens = [\n",
    "                np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "            ]\n",
    "            result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "            result = {k: round(v, 4) for k, v in result.items()}\n",
    "            results_dict.update(result)\n",
    "        return results_dict\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "###### CV LOOP PREP ########\n",
    "############################\n",
    "\n",
    "DATE_STR = \"20240327\"\n",
    "df = pd.read_csv(f\"../data/processed/conala/{DATE_STR}/all_drifts.csv\")\n",
    "df[\"t_batch\"] = df[\"time_batch\"]\n",
    "df.loc[df.rewritten_intent.isna(), \"rewritten_intent\"] = \"translate an ISO 8601 datetime string into a Python datetime object\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "\n",
    "full_train_idx = pd.Series(df.question_id.unique()).sample(n=1200, random_state=RS)\n",
    "test_idx = pd.Series(df.loc[~df.question_id.isin(full_train_idx), \"question_id\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m###### MODEL SETTINGS ######\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m      9\u001b[0m compute_metrics \u001b[38;5;241m=\u001b[39m compute_metric_with_params(tokenizer)\n\u001b[1;32m     13\u001b[0m TRAIN_ARGS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEQ_TRAINER_ARGS\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreports/upper_bound/results\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "############################\n",
    "###### MODEL SETTINGS ######\n",
    "############################\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "compute_metrics = compute_metric_with_params(tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"output_dir\"] = f'reports/upper_bound/results'\n",
    "TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"][\"logging_dir\"] = f'reports/upper_bound/logs'\n",
    "\n",
    "if not os.path.exists('reports/'): \n",
    "    os.mkdir('reports/')\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        **TRAIN_ARGS[\"SEQ_TRAINER_ARGS\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Preparing train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1133 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing val data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='467' max='852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [467/852 01:59 < 01:38, 3.90 it/s, Epoch 1.64/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.830400</td>\n",
       "      <td>3.665829</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.344600</td>\n",
       "      <td>0.344600</td>\n",
       "      <td>13.018200</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.814900</td>\n",
       "      <td>5581</td>\n",
       "      <td>6849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(FOLD_MODEL_PATH)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m fold_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     61\u001b[0m fold_trainer\u001b[38;5;241m.\u001b[39msave_model(FOLD_MODEL_PATH)\n\u001b[1;32m     63\u001b[0m val_ground_truths \u001b[38;5;241m=\u001b[39m val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_sequence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/dd_spek/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/dd_spek/lib/python3.11/site-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/dd_spek/lib/python3.11/site-packages/transformers/trainer.py:2781\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2779\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.conda/envs/dd_spek/lib/python3.11/site-packages/accelerate/accelerator.py:2001\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2001\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/dd_spek/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/dd_spek/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################\n",
    "######### CV TRAINING ######\n",
    "############################\n",
    "\n",
    "kf = KFold(n_splits=3, random_state=RS, shuffle=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(full_train_idx.values)):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Fold: {fold}\")\n",
    "    \n",
    "    fold_results= {}\n",
    "\n",
    "    train_df = df.loc[df.question_id.isin(full_train_idx.iloc[train_idx]),:]\n",
    "    val_df = df.loc[df.question_id.isin(full_train_idx.iloc[val_idx]),:]\n",
    "\n",
    "    train_df, train_dataset = prep_for_hf(train_df)\n",
    "    val_df, val_dataset = prep_for_hf(val_df)\n",
    "\n",
    "    print(\"Preparing train data\")\n",
    "\n",
    "    train_data = train_dataset.map(\n",
    "            lambda batch: batch_tokenize_preprocess(\n",
    "                batch,\n",
    "                tokenizer=tokenizer,\n",
    "                max_input_length=ENCODER_LENGTH,\n",
    "                max_output_length=DECODER_LENGTH,\n",
    "            ),\n",
    "            batch_size=4,\n",
    "            batched=True,\n",
    "            #remove_columns=train_dataset.column_names,\n",
    "        )\n",
    "\n",
    "    print(\"Preparing val data\")\n",
    "\n",
    "    val_data = val_dataset.map(\n",
    "            lambda batch: batch_tokenize_preprocess(\n",
    "                batch,\n",
    "                tokenizer=tokenizer,\n",
    "                max_input_length=ENCODER_LENGTH,\n",
    "                max_output_length=DECODER_LENGTH,\n",
    "            ),\n",
    "            batch_size=4,\n",
    "            batched=True,\n",
    "            #remove_columns=train_dataset.column_names,\n",
    "        )\n",
    "\n",
    "    fold_trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    FOLD_MODEL_PATH = \"reports/no_drift/fold_model\"\n",
    "    if not os.path.exists(FOLD_MODEL_PATH): \n",
    "        os.mkdir(FOLD_MODEL_PATH)\n",
    "\n",
    "    print(\"Training\")\n",
    "    fold_trainer.train()\n",
    "    fold_trainer.save_model(FOLD_MODEL_PATH)\n",
    "\n",
    "    val_ground_truths = val_data[\"output_sequence\"]\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    print(\"Inference\")\n",
    "    # Fine-Tuned\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(FOLD_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(FOLD_MODEL_PATH, skip_special_tokens=False)\n",
    "    fold_predictions_ft = generate_summary(val_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "    # Zero-Shot\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "    fold_predictions_zs = generate_summary(val_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "    # Rouge\n",
    "    print(\"Evaluation\")\n",
    "    fold_predictions_ft_rouge = rouge.compute(references=val_ground_truths, predictions=fold_predictions_ft, use_aggregator=False)[\"rouge1\"]\n",
    "    fold_predictions_zs_rouge = rouge.compute(references=val_ground_truths, predictions=fold_predictions_zs, use_aggregator=False)[\"rouge1\"]\n",
    "    \n",
    "    fold_results[\"input_sequence\"] = val_ground_truths\n",
    "    fold_results[\"fold_predictions_ft\"] = fold_predictions_ft\n",
    "    fold_results[\"fold_predictions_zs\"] = fold_predictions_zs\n",
    "    fold_results[\"fold_predictions_ft_rouge\"] = fold_predictions_ft_rouge\n",
    "    fold_results[\"fold_predictions_zs_rouge\"] = fold_predictions_zs_rouge\n",
    "\n",
    "    # Combine\n",
    "    fold_results = pd.DataFrame(fold_results)\n",
    "    fold_results[\"fold\"] = fold\n",
    "\n",
    "    if fold==0:\n",
    "        results = fold_results.copy()\n",
    "    else: \n",
    "        results = pd.concat([results, fold_results])\n",
    "\n",
    "results.to_csv(\"cv_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1263' max='1263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1263/1263 05:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Brevity Penalty</th>\n",
       "      <th>Length Ratio</th>\n",
       "      <th>Translation Length</th>\n",
       "      <th>Reference Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.777400</td>\n",
       "      <td>3.518943</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>13.946400</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>5992</td>\n",
       "      <td>6881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.957600</td>\n",
       "      <td>3.384997</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>0.400800</td>\n",
       "      <td>14.249500</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>6289</td>\n",
       "      <td>6881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.251900</td>\n",
       "      <td>3.390300</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>14.970400</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.972700</td>\n",
       "      <td>0.973100</td>\n",
       "      <td>6696</td>\n",
       "      <td>6881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/RDC/zinovyee.hub/.conda/envs/dd_spek/lib/python3.11/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######## FULL TRAINING #####\n",
    "############################\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "train_df = df.loc[df.question_id.isin(full_train_idx),:]\n",
    "test_df = df.loc[df.question_id.isin(test_idx),:]\n",
    "\n",
    "train_df, train_dataset = prep_for_hf(train_df)\n",
    "test_df, test_dataset = prep_for_hf(test_df)\n",
    "\n",
    "train_data = train_dataset.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_length=ENCODER_LENGTH,\n",
    "            max_output_length=DECODER_LENGTH,\n",
    "        ),\n",
    "        batch_size=4,\n",
    "        batched=True,\n",
    "        #remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "\n",
    "test_data = test_dataset.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_length=ENCODER_LENGTH,\n",
    "            max_output_length=DECODER_LENGTH,\n",
    "        ),\n",
    "        batch_size=4,\n",
    "        batched=True,\n",
    "        #remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "MODEL_PATH = f\"reports/upper_bound/saved_model\"\n",
    "if not os.path.exists(MODEL_PATH): \n",
    "    os.mkdir(MODEL_PATH)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "######### INFERENCE ########\n",
    "############################\n",
    "\n",
    "test_ground_truths = test_data[\"output_sequence\"]\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, skip_special_tokens=False)\n",
    "test_df[\"predictions_ft\"] = generate_summary(test_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, skip_special_tokens=False)\n",
    "test_df[\"predictions_zs\"] = generate_summary(test_data, model, tokenizer, encoder_max_length=ENCODER_LENGTH, decoder_max_length=DECODER_LENGTH)[1] \n",
    "\n",
    "############################\n",
    "######## EVALUATION ########\n",
    "############################\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "test_df[\"predictions_ft_rouge\"] = rouge.compute(references=test_ground_truths, predictions=test_df[\"predictions_ft\"].values, use_aggregator=False)[\"rouge1\"]\n",
    "test_df[\"predictions_zs_rouge\"] = rouge.compute(references=test_ground_truths, predictions=test_df[\"predictions_zs\"].values, use_aggregator=False)[\"rouge1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.278477188184076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4189334588957666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_df.predictions_zs_rouge.mean())\n",
    "test_df.predictions_ft_rouge.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"test_df.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd_spek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
